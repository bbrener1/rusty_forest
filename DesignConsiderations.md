# Design Condisderations for the Median Forest Program

This software is intended to perform several functions:

It is capable of performing imputation on missing data in order to calculate appropriate values for such. 

It is capable of performing multivariate regression on samples based on limited feature inputs.

It is capable of reporting back on feature relationships and interactions in the regression. 

The underlying function of the software is based on an implementation of decision tree based regression, both gradient boosted and plain random forest.

The random forest implementation is able to ignore dropped values during regression, and performs splits of data by minimizing Median Absolute Deviation from the Median (hereafter MAD) L1, L2, or .5L, the fractional L norm. 

Gradient boosting can be performed by summing error matrices or by over-representing error-prone features and samples. 

To aid interpretation, the program provides certain summary outputs that allow you to determine if appropriate norming, boosting, and ensemble integration was performed. 

The purpose of this program is interpretation of single-cell RNA Seq data, but it is appropriate for interpretation of other high-dimensional data. 

The general structure of the program is as follows:

The outer-most class is the Random Forest



# Random Forests:

A Random Forest contains

   * The matrix to be analyzed
   * Prototype Tree
   * Predictive Decision Trees
   * Decision Tree Thread Pool

Important methods

   * Method that initializes a forest and generates a prototype Tree and a Tree Thread Pool
   * Method that requests several Predictive Trees from a Tree Thread Pool
   * Method that generates predicted values for a matrix of samples

Random Forests are the outermost structure used for Random Forest-based regression.

 Random Forests are initialized with knowledge of the dimensions of the trees that they will contain and the data to be analyzed. Upon initialization a random forest will first generate a prototype Tree. 
 
 The prototype Tree contains all dimensions and all samples of data to be analyzed, and will serve as a template from which to generate other Trees. However the prototype tree has no splits, and has not had grow_branches() called on it. A prototype tree also owns a Feature Thread Pool, but said pool has only one thread. 
 
 The Random Forest will then initialize a Tree Thread Pool, and then request that Predictive Trees be generated from it. After Predictive Trees are generated, the Random Forest can store them and/or can use them to make predictions about a given sample based on data available about it.

# Tree Thread Pool:

 A Tree Thread Pool contains
   
   * Worker Threads
   * Reciever Channel for jobs
     
Important methods
   
   * Initialization method
   * Termination method


 A Tree Thread Pool is able to generate Predictive Trees on request. When a Tree Thread Pool is generated, it must be passed a prototype Tree. The Tree Thread Pool then generates several worker threads that contain clones of the prototype Tree, and which are able to generate predictive trees. The initializer for the Tree Thread Pool returns a mpsc Sender channel which is able to receive requests for Predictive Trees. The sender channel must receive a usize representing a unique ID for the tree to be generated and another sender channel that allows the thread pool to send back the generated tree. 

 Each worker thread is on a loop attempting to receive jobs from a Mutex that is shared among all worker threads. Each worker attempts to acquire a lock and then blocks the channel contained in the Mutex with a recv() call. Once a message containing a usize ID is received, the worker thread checks the ID. If ID is 0, the loop is broken and the thread terminates. If ID is not 0, a new Tree is derived from the prototype Tree with dimensions that are given by the Tree Thread Pool. Feature and Sample subsampling is random.

The terminate() method can be called on a sender to this pool in order to send thread termination signals until all Worker threads terminate. At this point the Pool should be deallocated as soon as it goes out of scope in the outer program. Otherwise, it may persist in memory. Unfortunately due to the fact that some underlying structures use Arc, it is difficult to implement the Drop trait for thread pools in order to allow them to be cleaned up automatically without calls to terminate(). 

 Usage note: Please note that the splitting algorithm used by Median Forests is deterministic so if subsampling settings are such that all samples and all features are subsampled, all trees generated by a Random Forest will be exactly identical. This generally leads to poor quality predictions. 

# Trees:

 Trees contain
     
   * Root Node
   * Feature Thread Pool Sender Channel
   * Drop Mode

Important methods
     
   * Derive (various versions)
   * grow_branches()
   * Strip (various versions)
   * Serialization (various versions)
   * Reconstitution 


 Each tree contains a subsampling of both rows and columns of the original matrix. The subsampled rows and columns are contained in a root node, which is the only node the tree has direct access to. A tree knows how to grow branches, ie it can send a request to the root node to perform the best possible split on itself, then the best possible split on the two resulting child nodes, and so on until reaching the per-leaf sample limit. 
 
 A Tree can be asked to Derive another Tree that contains some or all of the features and samples contained in the original Tree. Deriving a tree is generally cheaper than generating one from scratch because during derivation the sorting information is preserved, and so various Rank Table properties don't have to be re-computed. Derivation comes in two flavors: a new tree may be randomly derived (generally used as a subsampling procedure by Random Forest) or specific features and samples may be specified to be in the new tree (generally used during growing branches for trees and by the Gradient Boosted Forests)
 
 A Tree is a computational structure. Once a tree has grown branches, it is possible to produce a more compact version of a tree by calling strip(), which produces a Predictive Tree. A Predictive Tree has Stripped Nodes inside it, which means that it no longer contains the matrix of values that was used to generate the original Tree. Predictive Trees have a much smaller memory footprint than Trees.
 
 It is also possible to serialize a Tree. When serialize() is called on a Tree, it writes the a JSON string corresponding to its root node to its report address (specified at Random Forest initialization), appended with its unique ID. Calling serialize_compact() will instead write a corresponding Predictive Tree JSON string to the report address, appended with the unique tree ID and ".compact"



# Feature Thread Pool:

 Feature Thread Pool contains

   * Worker Threads
   * Reciever Channel for jobs

Important methods

   * A wrapper method to compute a set of medians and MADs for each job passed to the pool. Core method logic is in Rank Vector

 Feature Thread Pools are containers of Worker threads. Each pool contains a multiple in, single out channel locked with a Mutex. Each Worker contained in the pool continuously requests jobs from the channel. If the Mutex is unlocked and has a job, a Worker thread receives it.

Jobs:
         Jobs in the pool channel consist of a channel to pass back the solution to the underlying problem and Rank Vector (see below). The job consists of calling a method on the RV that consumes it and produces the medians and Median Absolute Deviations (MAD) from the Median of the vector if a set of samples is removed from it in a given order. This allows us to determine what the Median Absolute Deviation from the Median would be given the split of that feature by some draw order. The draw orders given to each job are usually denoting that the underlying matrix was sorted by another feature. 

After a vector has been consumed, manual_reset() is called on it to restore the original internal structure in-place. This allows the Worker to send back the Vector and the resulting split values. A vector has to be reset to avoid excessive allocations of memory during the cloning and consumption of Rank Vectors during the splitting procedure. By using an in-place reset, the entire data structure is moved between threads, never undergoing reallocation after it has been initially generated. 

Due to the processing that needs to be done outside of the Feature Thread Pool by a Rank Table that is undergoing a Split, generally Feature Thread Pools run into diminishing returns when they have more than approximately 10 workers. Because of this, the Tree Thread Pool was implemented that parallelizes computations of multiple Trees, and each Tree is allocated up to 10 threads for use by the Feature Thread Pool. 

In practice generally Median Forests will produce load across about 80-90% of the maximum number of threads, though this varies over time. 

 Worker threads are simple anonymous threads kept in a vector in the pool, requesting jobs on loop from the channel.
 
 # Node:
 
 Nodes are basic units of a decision Tree, and know which samples they contain, which features they contain, the value of each feature they contain for each sample they contain, how to split themselves into child nodes, and which Feature Thread Pool to send jobs to. 
 
A Node Contains:

   * Node ID
   * Feature (Optional)
   * Split (Optional)
   * Input and Output Rank Tables
   * Feature Thread Pool Channel
   * Medians
   * Dispersions
   * Gains

Important methods

   * Parallel Best Split
   * Derive (Various flavors)
 
When a Node is generated, it is passed an input and an output matrix. These two matrices generate two Rank Tables, the Input and Output Rank Tables. When a Node is told to perform a best split, it generates a draw order for each feature in the Input Rank Table. Each draw order represents "sorting" a table by that feature, eg it's similar to argsort(). 

Important: default (and currently only possible) behavior is to ignore "dropped" values for the purposes of computing medians, dispersions, and not to include them in draw orders. Functionally, this means that for any feature, you are sorting ONLY by non-dropped values. For the purposes of calculating dispersion of the split samples, any samples for which a feature value is dropped are included in both branches of the tree. However when the child nodes are derive, all those samples are dropped. I am aware of the questionable nature of this minimization procedure and will update accordingly at a later date. 

After this, the Node can ask the Output Rank Table to produce summary dispersion values for each possible split of the Rank Table along a given feature. 

After finding the best possible split along each feature, the Node finds the best feature to split by.

After finding the best feature and the best split for that feature, the Node identifies which samples should belong to each of two child Nodes when splitting the samples by the best feature, and derives two child Nodes with identical selections of input and output features, but a smaller selection of samples. 

